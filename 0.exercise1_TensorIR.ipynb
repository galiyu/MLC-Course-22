{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单练习\n",
    "\n",
    "作为练习，尝试不同的 j_factor 选择，看看它们如何影响代码的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T\n",
    "import numpy as np\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModule:\n",
    "    @T.prim_func\n",
    "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
    "        # function attr dict\n",
    "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
    "        # body\n",
    "        # with T.block(\"root\")\n",
    "        Y = T.alloc_buffer([128, 128], dtype=\"float32\")\n",
    "        for i, j, k in T.grid(128, 128, 128):\n",
    "            with T.block(\"Y\"):\n",
    "                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n",
    "                T.reads(A[vi, vk], B[vk, vj])\n",
    "                T.writes(Y[vi, vj])\n",
    "                with T.init():\n",
    "                    Y[vi, vj] = T.float32(0)\n",
    "                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
    "        for i, j in T.grid(128, 128):\n",
    "            with T.block(\"C\"):\n",
    "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "                T.reads(Y[vi, vj])\n",
    "                T.writes(C[vi, vj])\n",
    "                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time cost of transformed mod_transformed 0.000471282 sec\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># from tvm.script import tir as T</span>\n",
       "<span class=\"nd\">@tvm</span><span class=\"o\">.</span><span class=\"n\">script</span><span class=\"o\">.</span><span class=\"n\">ir_module</span>\n",
       "<span class=\"k\">class</span> <span class=\"nc\">Module</span><span class=\"p\">:</span>\n",
       "    <span class=\"nd\">@T</span><span class=\"o\">.</span><span class=\"n\">prim_func</span>\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">mm_relu</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">Buffer</span><span class=\"p\">[(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">),</span> <span class=\"s2\">&quot;float32&quot;</span><span class=\"p\">],</span> <span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">Buffer</span><span class=\"p\">[(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">),</span> <span class=\"s2\">&quot;float32&quot;</span><span class=\"p\">],</span> <span class=\"n\">C</span><span class=\"p\">:</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">Buffer</span><span class=\"p\">[(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">),</span> <span class=\"s2\">&quot;float32&quot;</span><span class=\"p\">])</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n",
       "        <span class=\"c1\"># function attr dict</span>\n",
       "        <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">func_attr</span><span class=\"p\">({</span><span class=\"s2\">&quot;global_symbol&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;mm_relu&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;tir.noalias&quot;</span><span class=\"p\">:</span> <span class=\"kc\">True</span><span class=\"p\">})</span>\n",
       "        <span class=\"c1\"># body</span>\n",
       "        <span class=\"c1\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">alloc_buffer</span><span class=\"p\">([</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"s2\">&quot;float32&quot;</span><span class=\"p\">)</span>\n",
       "        <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">j_0</span> <span class=\"ow\">in</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">):</span>\n",
       "            <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">j_1</span> <span class=\"ow\">in</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">):</span>\n",
       "                <span class=\"k\">with</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">block</span><span class=\"p\">(</span><span class=\"s2\">&quot;Y&quot;</span><span class=\"p\">):</span>\n",
       "                    <span class=\"n\">vi</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"o\">.</span><span class=\"n\">spatial</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">)</span>\n",
       "                    <span class=\"n\">vj</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"o\">.</span><span class=\"n\">spatial</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">j_0</span> <span class=\"o\">*</span> <span class=\"mi\">8</span> <span class=\"o\">+</span> <span class=\"n\">j_1</span><span class=\"p\">)</span>\n",
       "                    <span class=\"n\">vk</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"o\">.</span><span class=\"n\">reduce</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">)</span>\n",
       "                    <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">reads</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vk</span><span class=\"p\">],</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">vk</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">])</span>\n",
       "                    <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">writes</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">])</span>\n",
       "                    <span class=\"k\">with</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">init</span><span class=\"p\">():</span>\n",
       "                        <span class=\"n\">Y</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n",
       "                    <span class=\"n\">Y</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vk</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">vk</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">]</span>\n",
       "            <span class=\"k\">for</span> <span class=\"n\">ax0</span> <span class=\"ow\">in</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">serial</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">):</span>\n",
       "                <span class=\"k\">with</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">block</span><span class=\"p\">(</span><span class=\"s2\">&quot;C&quot;</span><span class=\"p\">):</span>\n",
       "                    <span class=\"n\">vi</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"o\">.</span><span class=\"n\">spatial</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">)</span>\n",
       "                    <span class=\"n\">vj</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"o\">.</span><span class=\"n\">spatial</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">j_0</span> <span class=\"o\">*</span> <span class=\"mi\">8</span> <span class=\"o\">+</span> <span class=\"n\">ax0</span><span class=\"p\">)</span>\n",
       "                    <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">reads</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">])</span>\n",
       "                    <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">writes</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">])</span>\n",
       "                    <span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">max</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">],</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} from tvm.script import tir as T}\n",
       "\\PY{n+nd}{@tvm}\\PY{o}{.}\\PY{n}{script}\\PY{o}{.}\\PY{n}{ir\\PYZus{}module}\n",
       "\\PY{k}{class} \\PY{n+nc}{Module}\\PY{p}{:}\n",
       "    \\PY{n+nd}{@T}\\PY{o}{.}\\PY{n}{prim\\PYZus{}func}\n",
       "    \\PY{k}{def} \\PY{n+nf}{mm\\PYZus{}relu}\\PY{p}{(}\\PY{n}{A}\\PY{p}{:} \\PY{n}{T}\\PY{o}{.}\\PY{n}{Buffer}\\PY{p}{[}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{l+m+mi}{128}\\PY{p}{)}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{float32}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{,} \\PY{n}{B}\\PY{p}{:} \\PY{n}{T}\\PY{o}{.}\\PY{n}{Buffer}\\PY{p}{[}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{l+m+mi}{128}\\PY{p}{)}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{float32}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{,} \\PY{n}{C}\\PY{p}{:} \\PY{n}{T}\\PY{o}{.}\\PY{n}{Buffer}\\PY{p}{[}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{l+m+mi}{128}\\PY{p}{)}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{float32}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{)} \\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZgt{}} \\PY{k+kc}{None}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{} function attr dict}\n",
       "        \\PY{n}{T}\\PY{o}{.}\\PY{n}{func\\PYZus{}attr}\\PY{p}{(}\\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{global\\PYZus{}symbol}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{mm\\PYZus{}relu}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{tir.noalias}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{k+kc}{True}\\PY{p}{\\PYZcb{}}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{} body}\n",
       "        \\PY{c+c1}{\\PYZsh{} with T.block(\\PYZdq{}root\\PYZdq{})}\n",
       "        \\PY{n}{Y} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{alloc\\PYZus{}buffer}\\PY{p}{(}\\PY{p}{[}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{l+m+mi}{128}\\PY{p}{]}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{float32}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\n",
       "        \\PY{k}{for} \\PY{n}{i}\\PY{p}{,} \\PY{n}{j\\PYZus{}0} \\PY{o+ow}{in} \\PY{n}{T}\\PY{o}{.}\\PY{n}{grid}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{l+m+mi}{16}\\PY{p}{)}\\PY{p}{:}\n",
       "            \\PY{k}{for} \\PY{n}{k}\\PY{p}{,} \\PY{n}{j\\PYZus{}1} \\PY{o+ow}{in} \\PY{n}{T}\\PY{o}{.}\\PY{n}{grid}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{l+m+mi}{8}\\PY{p}{)}\\PY{p}{:}\n",
       "                \\PY{k}{with} \\PY{n}{T}\\PY{o}{.}\\PY{n}{block}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{Y}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{:}\n",
       "                    \\PY{n}{vi} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{axis}\\PY{o}{.}\\PY{n}{spatial}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{n}{i}\\PY{p}{)}\n",
       "                    \\PY{n}{vj} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{axis}\\PY{o}{.}\\PY{n}{spatial}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{n}{j\\PYZus{}0} \\PY{o}{*} \\PY{l+m+mi}{8} \\PY{o}{+} \\PY{n}{j\\PYZus{}1}\\PY{p}{)}\n",
       "                    \\PY{n}{vk} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{axis}\\PY{o}{.}\\PY{n}{reduce}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{n}{k}\\PY{p}{)}\n",
       "                    \\PY{n}{T}\\PY{o}{.}\\PY{n}{reads}\\PY{p}{(}\\PY{n}{A}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vk}\\PY{p}{]}\\PY{p}{,} \\PY{n}{B}\\PY{p}{[}\\PY{n}{vk}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\\PY{p}{)}\n",
       "                    \\PY{n}{T}\\PY{o}{.}\\PY{n}{writes}\\PY{p}{(}\\PY{n}{Y}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\\PY{p}{)}\n",
       "                    \\PY{k}{with} \\PY{n}{T}\\PY{o}{.}\\PY{n}{init}\\PY{p}{(}\\PY{p}{)}\\PY{p}{:}\n",
       "                        \\PY{n}{Y}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{(}\\PY{l+m+mi}{0}\\PY{p}{)}\n",
       "                    \\PY{n}{Y}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]} \\PY{o}{=} \\PY{n}{Y}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]} \\PY{o}{+} \\PY{n}{A}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vk}\\PY{p}{]} \\PY{o}{*} \\PY{n}{B}\\PY{p}{[}\\PY{n}{vk}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\n",
       "            \\PY{k}{for} \\PY{n}{ax0} \\PY{o+ow}{in} \\PY{n}{T}\\PY{o}{.}\\PY{n}{serial}\\PY{p}{(}\\PY{l+m+mi}{8}\\PY{p}{)}\\PY{p}{:}\n",
       "                \\PY{k}{with} \\PY{n}{T}\\PY{o}{.}\\PY{n}{block}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{C}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{:}\n",
       "                    \\PY{n}{vi} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{axis}\\PY{o}{.}\\PY{n}{spatial}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{n}{i}\\PY{p}{)}\n",
       "                    \\PY{n}{vj} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{axis}\\PY{o}{.}\\PY{n}{spatial}\\PY{p}{(}\\PY{l+m+mi}{128}\\PY{p}{,} \\PY{n}{j\\PYZus{}0} \\PY{o}{*} \\PY{l+m+mi}{8} \\PY{o}{+} \\PY{n}{ax0}\\PY{p}{)}\n",
       "                    \\PY{n}{T}\\PY{o}{.}\\PY{n}{reads}\\PY{p}{(}\\PY{n}{Y}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\\PY{p}{)}\n",
       "                    \\PY{n}{T}\\PY{o}{.}\\PY{n}{writes}\\PY{p}{(}\\PY{n}{C}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\\PY{p}{)}\n",
       "                    \\PY{n}{C}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{max}\\PY{p}{(}\\PY{n}{Y}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\\PY{p}{,} \\PY{n}{T}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{(}\\PY{l+m+mi}{0}\\PY{p}{)}\\PY{p}{)}\n",
       "    \n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# from tvm.script import tir as T\n",
       "@tvm.script.ir_module\n",
       "class Module:\n",
       "    @T.prim_func\n",
       "    def mm_relu(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
       "        # function attr dict\n",
       "        T.func_attr({\"global_symbol\": \"mm_relu\", \"tir.noalias\": True})\n",
       "        # body\n",
       "        # with T.block(\"root\")\n",
       "        Y = T.alloc_buffer([128, 128], dtype=\"float32\")\n",
       "        for i, j_0 in T.grid(128, 16):\n",
       "            for k, j_1 in T.grid(128, 8):\n",
       "                with T.block(\"Y\"):\n",
       "                    vi = T.axis.spatial(128, i)\n",
       "                    vj = T.axis.spatial(128, j_0 * 8 + j_1)\n",
       "                    vk = T.axis.reduce(128, k)\n",
       "                    T.reads(A[vi, vk], B[vk, vj])\n",
       "                    T.writes(Y[vi, vj])\n",
       "                    with T.init():\n",
       "                        Y[vi, vj] = T.float32(0)\n",
       "                    Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n",
       "            for ax0 in T.serial(8):\n",
       "                with T.block(\"C\"):\n",
       "                    vi = T.axis.spatial(128, i)\n",
       "                    vj = T.axis.spatial(128, j_0 * 8 + ax0)\n",
       "                    T.reads(Y[vi, vj])\n",
       "                    T.writes(C[vi, vj])\n",
       "                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))\n",
       "    "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(mod, jfactor):\n",
    "    sch = tvm.tir.Schedule(mod)\n",
    "    block_Y = sch.get_block(\"Y\", func_name=\"mm_relu\")\n",
    "    i, j, k = sch.get_loops(block_Y)\n",
    "    j0, j1 = sch.split(j, factors=[None, jfactor])\n",
    "    sch.reorder(j0, k, j1)\n",
    "    block_C = sch.get_block(\"C\", \"mm_relu\")\n",
    "    sch.reverse_compute_at(block_C, j0)\n",
    "    return sch.mod\n",
    "\n",
    "mod_transformed = transform(MyModule, jfactor=8)\n",
    "\n",
    "dtype = \"float32\"\n",
    "a_np = np.random.rand(128, 128).astype(dtype)\n",
    "b_np = np.random.rand(128, 128).astype(dtype)\n",
    "\n",
    "a_nd = tvm.nd.array(a_np)\n",
    "b_nd = tvm.nd.array(b_np)\n",
    "c_nd = tvm.nd.empty((128, 128), dtype=\"float32\")\n",
    "\n",
    "rt_lib_transformed = tvm.build(mod_transformed, \"llvm\")\n",
    "f_timer_transformed = rt_lib_transformed.time_evaluator(\"mm_relu\", tvm.cpu())\n",
    "print(\"Time cost of transformed mod_transformed %g sec\" % f_timer_transformed(a_nd, b_nd, c_nd).mean)\n",
    "# display the code below\n",
    "IPython.display.Code(mod_transformed.script(), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorIR练习\n",
    "\n",
    "- 如何编写\n",
    "    - 示例：逐位相加\n",
    "    - 1.广播加法\n",
    "    - 2.二维卷积\n",
    "\n",
    "- 如何变换\n",
    "    - 示例：并行化，向量化与循环展开\n",
    "    - 3.变换批量矩阵乘法程序\n",
    "    - 构建与评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何编写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================numpy实现====================\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "[[16 15 14 13]\n",
      " [12 11 10  9]\n",
      " [ 8  7  6  5]\n",
      " [ 4  3  2  1]]\n",
      "[[16 16 16 16]\n",
      " [16 16 16 16]\n",
      " [16 16 16 16]\n",
      " [16 16 16 16]]\n",
      "====================low-level numpy===================\n",
      "[[16 16 16 16]\n",
      " [16 16 16 16]\n",
      " [16 16 16 16]\n",
      " [16 16 16 16]]\n",
      "====================TensorIR===================\n"
     ]
    }
   ],
   "source": [
    "#=======================================\n",
    "print(\"===================numpy实现====================\")\n",
    "# 首先用numpy实现\n",
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(16, 0, -1).reshape(4, 4)\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# numpy version\n",
    "c_np = a + b\n",
    "print(c_np)\n",
    "\n",
    "#========================================\n",
    "print(\"====================low-level numpy===================\")\n",
    "# 在我们直接编写 TensorIR 之前，我们应该首先将高级计算抽象（例如，ndarray + ndarray）\n",
    "# 转换为低级 Python 实现（具有元素访问和操作的循环的标准）。\n",
    "\n",
    "# 值得注意的是，输出数组（或缓冲区）的初始值并不总是 0。\n",
    "# 我们需要在我们的实现中编写或初始化它，这对于归约运算符（例如 matmul 和 conv）很重要。\n",
    "# low-level numpy version\n",
    "def lnumpy_add(a: np.ndarray, b: np.ndarray, c: np.ndarray):\n",
    "  for i in range(4):\n",
    "    for j in range(4):\n",
    "      c[i, j] = a[i, j] + b[i, j]\n",
    "c_lnumpy = np.empty((4, 4), dtype=np.int64)\n",
    "lnumpy_add(a, b, c_lnumpy)\n",
    "print(c_lnumpy)\n",
    "\n",
    "\n",
    "#========================================\n",
    "print(\"====================TensorIR===================\")\n",
    "# TensorIR version\n",
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(A: T.Buffer[(4, 4), \"int64\"],\n",
    "          B: T.Buffer[(4, 4), \"int64\"],\n",
    "          C: T.Buffer[(4, 4), \"int64\"]):\n",
    "    T.func_attr({\"global_symbol\": \"add\"})\n",
    "    for i, j in T.grid(4, 4):\n",
    "      with T.block(\"C\"):\n",
    "        vi = T.axis.spatial(4, i)\n",
    "        vj = T.axis.spatial(4, j)\n",
    "        C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================numpy实现====================\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "[4 3 2 1]\n",
      "[[ 4  4  4  4]\n",
      " [ 8  8  8  8]\n",
      " [12 12 12 12]\n",
      " [16 16 16 16]]\n",
      "===================TensorIR====================\n",
      "[[ 4  4  4  4]\n",
      " [ 8  8  8  8]\n",
      " [12 12 12 12]\n",
      " [16 16 16 16]]\n"
     ]
    }
   ],
   "source": [
    "# 练习 1：广播加法\n",
    "#=======================================\n",
    "print(\"===================numpy实现====================\")\n",
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(4, 0, -1).reshape(4)\n",
    "print(a)\n",
    "print(b)\n",
    "# numpy version\n",
    "c_np = a + b\n",
    "print(c_np)\n",
    "\n",
    "#=======================================\n",
    "print(\"===================TensorIR====================\")\n",
    "\n",
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(\n",
    "    A: T.Buffer[(4, 4), \"int64\"],\n",
    "    B: T.Buffer[(4), \"int64\"],\n",
    "    C: T.Buffer[(4, 4), \"int64\"]\n",
    "  ):\n",
    "    T.func_attr({\"global_symbol\": \"add\", \"tir.noalias\": True})\n",
    "    for i, j in T.grid(4, 4):\n",
    "      with T.block(\"C\"):\n",
    "        vi = T.axis.spatial(4, i)\n",
    "        vj = T.axis.spatial(4, j)\n",
    "        C[vi, vj] = A[vi, vj] + B[vj]\n",
    "\n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "print(c_tvm.numpy())\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================torch实现====================\n",
      "[[[[ 474  510  546  582  618  654]\n",
      "   [ 762  798  834  870  906  942]\n",
      "   [1050 1086 1122 1158 1194 1230]\n",
      "   [1338 1374 1410 1446 1482 1518]\n",
      "   [1626 1662 1698 1734 1770 1806]\n",
      "   [1914 1950 1986 2022 2058 2094]]\n",
      "\n",
      "  [[1203 1320 1437 1554 1671 1788]\n",
      "   [2139 2256 2373 2490 2607 2724]\n",
      "   [3075 3192 3309 3426 3543 3660]\n",
      "   [4011 4128 4245 4362 4479 4596]\n",
      "   [4947 5064 5181 5298 5415 5532]\n",
      "   [5883 6000 6117 6234 6351 6468]]]]\n",
      "===================TensorIR====================\n"
     ]
    }
   ],
   "source": [
    "# 练习 2：二维卷积\n",
    "#=======================================\n",
    "print(\"===================torch实现====================\")\n",
    "stride = 1\n",
    "pad = 0\n",
    "N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "OUT_H, OUT_W = (H - K + pad*2)//stride + 1, (W - K + pad*2)//stride + 1\n",
    "data = np.arange(N*CI*H*W).reshape(N, CI, H, W)\n",
    "weight = np.arange(CO*CI*K*K).reshape(CO, CI, K, K)\n",
    "# print(data)\n",
    "# print(weight)\n",
    "# torch version\n",
    "import torch\n",
    "data_torch = torch.Tensor(data)\n",
    "weight_torch = torch.Tensor(weight)\n",
    "conv_torch = torch.nn.functional.conv2d(data_torch, weight_torch)\n",
    "conv_torch = conv_torch.numpy().astype(np.int64)\n",
    "print(conv_torch)\n",
    "\n",
    "\n",
    "#=======================================\n",
    "print(\"===================TensorIR====================\")\n",
    "\n",
    "@tvm.script.ir_module\n",
    "class MyConv:\n",
    "  \n",
    "  @T.prim_func\n",
    "  def conv(\n",
    "    data: T.Buffer[(1, 1, 8, 8), \"int64\"],\n",
    "    weight: T.Buffer[(2, 1, 3, 3), \"int64\"],\n",
    "    output: T.Buffer[(1, 2, 6, 6), \"int64\"]\n",
    "  ):\n",
    "    T.func_attr({\"global_symbol\": \"conv\", \"tir.noalias\": True})\n",
    "    # TODO\n",
    "    for b, k, i, j, di, dj, q in T.grid(N, CO, OUT_H, OUT_W, K, K, CI):\n",
    "      with T.block(\"output\"):\n",
    "        vdi = T.axis.reduce(3, di)\n",
    "        vdj = T.axis.reduce(3, dj)\n",
    "        vq = T.axis.reduce(1, q)\n",
    "        vb = T.axis.spatial(1, b)\n",
    "        vk = T.axis.spatial(2, k)\n",
    "        vi = T.axis.spatial(6, i)\n",
    "        vj = T.axis.spatial(6, j)\n",
    "        with T.init():\n",
    "          output[vb, vk, vi, vj] = T.int64(0)\n",
    "        output[vb, vk, vi, vj] = output[vb, vk, vi, vj] + data[vb, vq, vi+vdi, vj+vdj] * weight[vk,vq,vdi,vdj]\n",
    "\n",
    "rt_lib = tvm.build(MyConv, target=\"llvm\")\n",
    "data_tvm = tvm.nd.array(data)\n",
    "weight_tvm = tvm.nd.array(weight)\n",
    "conv_tvm = tvm.nd.array(np.empty((N, CO, OUT_H, OUT_W), dtype=np.int64))\n",
    "rt_lib[\"conv\"](data_tvm, weight_tvm, conv_tvm)\n",
    "np.testing.assert_allclose(conv_tvm.numpy(), conv_torch, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 经验\n",
    "\n",
    "这是使用 NCHW 布局的卷积的数学定义：\n",
    "$$Conv[b, k, i, j] =\n",
    "    \\sum_{di, dj, q} A[b, q, strides * i + di, strides * j + dj] * W[k, q, di, dj],$$\n",
    "其中，`A` 是输入张量，`W` 是权重张量，`b` 是批次索引，`k` 是输出通道，`i` 和 `j` 是图像高度和宽度的索引，`di` 和 `dj` 是权重的索引，`q` 是输入通道，`strides` 是过滤器窗口的步幅。\n",
    "\n",
    "- 1.我们的目标是得到结果矩阵，根据结果矩阵的维度得到空间轴(b, k, i, j)，根据计算过程中的循环嵌套得出规约轴(di, dj, q)。根据数学公式带入即可\n",
    "- 2.不要忘记初始化！！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 如何变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># from tvm.script import tir as T</span>\n",
       "<span class=\"nd\">@tvm</span><span class=\"o\">.</span><span class=\"n\">script</span><span class=\"o\">.</span><span class=\"n\">ir_module</span>\n",
       "<span class=\"k\">class</span> <span class=\"nc\">Module</span><span class=\"p\">:</span>\n",
       "    <span class=\"nd\">@T</span><span class=\"o\">.</span><span class=\"n\">prim_func</span>\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">:</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">Buffer</span><span class=\"p\">[(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span> <span class=\"s2\">&quot;int64&quot;</span><span class=\"p\">],</span> <span class=\"n\">B</span><span class=\"p\">:</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">Buffer</span><span class=\"p\">[(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span> <span class=\"s2\">&quot;int64&quot;</span><span class=\"p\">],</span> <span class=\"n\">C</span><span class=\"p\">:</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">Buffer</span><span class=\"p\">[(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span> <span class=\"s2\">&quot;int64&quot;</span><span class=\"p\">])</span> <span class=\"o\">-&gt;</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n",
       "        <span class=\"c1\"># function attr dict</span>\n",
       "        <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">func_attr</span><span class=\"p\">({</span><span class=\"s2\">&quot;global_symbol&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;add&quot;</span><span class=\"p\">})</span>\n",
       "        <span class=\"c1\"># body</span>\n",
       "        <span class=\"c1\"># with T.block(&quot;root&quot;)</span>\n",
       "        <span class=\"k\">for</span> <span class=\"n\">i_0</span> <span class=\"ow\">in</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">parallel</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">):</span>\n",
       "            <span class=\"k\">for</span> <span class=\"n\">i_1</span> <span class=\"ow\">in</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">unroll</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">):</span>\n",
       "                <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">vectorized</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">):</span>\n",
       "                    <span class=\"k\">with</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">block</span><span class=\"p\">(</span><span class=\"s2\">&quot;C&quot;</span><span class=\"p\">):</span>\n",
       "                        <span class=\"n\">vi</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"o\">.</span><span class=\"n\">spatial</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">i_0</span> <span class=\"o\">*</span> <span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"n\">i_1</span><span class=\"p\">)</span>\n",
       "                        <span class=\"n\">vj</span> <span class=\"o\">=</span> <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"o\">.</span><span class=\"n\">spatial</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">j</span><span class=\"p\">)</span>\n",
       "                        <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">reads</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">],</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">])</span>\n",
       "                        <span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">writes</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">])</span>\n",
       "                        <span class=\"n\">C</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">B</span><span class=\"p\">[</span><span class=\"n\">vi</span><span class=\"p\">,</span> <span class=\"n\">vj</span><span class=\"p\">]</span>\n",
       "    \n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} from tvm.script import tir as T}\n",
       "\\PY{n+nd}{@tvm}\\PY{o}{.}\\PY{n}{script}\\PY{o}{.}\\PY{n}{ir\\PYZus{}module}\n",
       "\\PY{k}{class} \\PY{n+nc}{Module}\\PY{p}{:}\n",
       "    \\PY{n+nd}{@T}\\PY{o}{.}\\PY{n}{prim\\PYZus{}func}\n",
       "    \\PY{k}{def} \\PY{n+nf}{add}\\PY{p}{(}\\PY{n}{A}\\PY{p}{:} \\PY{n}{T}\\PY{o}{.}\\PY{n}{Buffer}\\PY{p}{[}\\PY{p}{(}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{4}\\PY{p}{)}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{int64}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{,} \\PY{n}{B}\\PY{p}{:} \\PY{n}{T}\\PY{o}{.}\\PY{n}{Buffer}\\PY{p}{[}\\PY{p}{(}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{4}\\PY{p}{)}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{int64}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{,} \\PY{n}{C}\\PY{p}{:} \\PY{n}{T}\\PY{o}{.}\\PY{n}{Buffer}\\PY{p}{[}\\PY{p}{(}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{l+m+mi}{4}\\PY{p}{)}\\PY{p}{,} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{int64}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{]}\\PY{p}{)} \\PY{o}{\\PYZhy{}}\\PY{o}{\\PYZgt{}} \\PY{k+kc}{None}\\PY{p}{:}\n",
       "        \\PY{c+c1}{\\PYZsh{} function attr dict}\n",
       "        \\PY{n}{T}\\PY{o}{.}\\PY{n}{func\\PYZus{}attr}\\PY{p}{(}\\PY{p}{\\PYZob{}}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{global\\PYZus{}symbol}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{:} \\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{add}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{\\PYZcb{}}\\PY{p}{)}\n",
       "        \\PY{c+c1}{\\PYZsh{} body}\n",
       "        \\PY{c+c1}{\\PYZsh{} with T.block(\\PYZdq{}root\\PYZdq{})}\n",
       "        \\PY{k}{for} \\PY{n}{i\\PYZus{}0} \\PY{o+ow}{in} \\PY{n}{T}\\PY{o}{.}\\PY{n}{parallel}\\PY{p}{(}\\PY{l+m+mi}{2}\\PY{p}{)}\\PY{p}{:}\n",
       "            \\PY{k}{for} \\PY{n}{i\\PYZus{}1} \\PY{o+ow}{in} \\PY{n}{T}\\PY{o}{.}\\PY{n}{unroll}\\PY{p}{(}\\PY{l+m+mi}{2}\\PY{p}{)}\\PY{p}{:}\n",
       "                \\PY{k}{for} \\PY{n}{j} \\PY{o+ow}{in} \\PY{n}{T}\\PY{o}{.}\\PY{n}{vectorized}\\PY{p}{(}\\PY{l+m+mi}{4}\\PY{p}{)}\\PY{p}{:}\n",
       "                    \\PY{k}{with} \\PY{n}{T}\\PY{o}{.}\\PY{n}{block}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{C}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{:}\n",
       "                        \\PY{n}{vi} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{axis}\\PY{o}{.}\\PY{n}{spatial}\\PY{p}{(}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{n}{i\\PYZus{}0} \\PY{o}{*} \\PY{l+m+mi}{2} \\PY{o}{+} \\PY{n}{i\\PYZus{}1}\\PY{p}{)}\n",
       "                        \\PY{n}{vj} \\PY{o}{=} \\PY{n}{T}\\PY{o}{.}\\PY{n}{axis}\\PY{o}{.}\\PY{n}{spatial}\\PY{p}{(}\\PY{l+m+mi}{4}\\PY{p}{,} \\PY{n}{j}\\PY{p}{)}\n",
       "                        \\PY{n}{T}\\PY{o}{.}\\PY{n}{reads}\\PY{p}{(}\\PY{n}{A}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\\PY{p}{,} \\PY{n}{B}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\\PY{p}{)}\n",
       "                        \\PY{n}{T}\\PY{o}{.}\\PY{n}{writes}\\PY{p}{(}\\PY{n}{C}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\\PY{p}{)}\n",
       "                        \\PY{n}{C}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]} \\PY{o}{=} \\PY{n}{A}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]} \\PY{o}{+} \\PY{n}{B}\\PY{p}{[}\\PY{n}{vi}\\PY{p}{,} \\PY{n}{vj}\\PY{p}{]}\n",
       "    \n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# from tvm.script import tir as T\n",
       "@tvm.script.ir_module\n",
       "class Module:\n",
       "    @T.prim_func\n",
       "    def add(A: T.Buffer[(4, 4), \"int64\"], B: T.Buffer[(4, 4), \"int64\"], C: T.Buffer[(4, 4), \"int64\"]) -> None:\n",
       "        # function attr dict\n",
       "        T.func_attr({\"global_symbol\": \"add\"})\n",
       "        # body\n",
       "        # with T.block(\"root\")\n",
       "        for i_0 in T.parallel(2):\n",
       "            for i_1 in T.unroll(2):\n",
       "                for j in T.vectorized(4):\n",
       "                    with T.block(\"C\"):\n",
       "                        vi = T.axis.spatial(4, i_0 * 2 + i_1)\n",
       "                        vj = T.axis.spatial(4, j)\n",
       "                        T.reads(A[vi, vj], B[vi, vj])\n",
       "                        T.writes(C[vi, vj])\n",
       "                        C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
       "    "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例：并行化、向量化与循环展开\n",
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(A: T.Buffer[(4, 4), \"int64\"],\n",
    "          B: T.Buffer[(4, 4), \"int64\"],\n",
    "          C: T.Buffer[(4, 4), \"int64\"]):\n",
    "    T.func_attr({\"global_symbol\": \"add\"})\n",
    "    for i, j in T.grid(4, 4):\n",
    "      with T.block(\"C\"):\n",
    "        vi = T.axis.spatial(4, i)\n",
    "        vj = T.axis.spatial(4, j)\n",
    "        C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
    "\n",
    "sch = tvm.tir.Schedule(MyAdd)\n",
    "block = sch.get_block(\"C\", func_name=\"add\")\n",
    "i, j = sch.get_loops(block)\n",
    "i0, i1 = sch.split(i, factors=[2, 2])\n",
    "sch.parallel(i0)\n",
    "sch.unroll(i1)\n",
    "sch.vectorize(j)\n",
    "import IPython\n",
    "IPython.display.Code(sch.mod.script(), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================low-level numpy实现====================\n"
     ]
    }
   ],
   "source": [
    "# 练习 3：变换批量矩阵乘法程序\n",
    "#=======================================\n",
    "print(\"===================low-level numpy实现====================\")\n",
    "dtype = \"float32\"\n",
    "a_np = np.random.rand(16, 128, 128).astype(dtype)\n",
    "b_np = np.random.rand(16, 128, 128).astype(dtype)\n",
    "\n",
    "def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
    "    Y = np.empty((16, 128, 128), dtype=\"float32\")\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                for k in range(128):\n",
    "                    if k == 0:\n",
    "                        Y[n, i, j] = 0\n",
    "                    Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                C[n, i, j] = max(Y[n, i, j], 0)\n",
    "\n",
    "c_np = np.empty((16, 128, 128), dtype=dtype)\n",
    "lnumpy_mm_relu_v2(a_np, b_np, c_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================TensorIR实现====================\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n, i, j, k in T.grid(16, 128, 128, 128):\n",
      "            with T.block(\"Y\"):\n",
      "                vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
      "                T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                T.writes(Y[vn, vi, vj])\n",
      "                with T.init():\n",
      "                    Y[vn, vi, vj] = T.float32(0)\n",
      "                Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "        for n, i, j in T.grid(16, 128, 128):\n",
      "            with T.block(\"C\"):\n",
      "                vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                T.reads(Y[vn, vi, vj])\n",
      "                T.writes(C[vn, vi, vj])\n",
      "                C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#=======================================\n",
    "print(\"===================TensorIR实现====================\")\n",
    "@tvm.script.ir_module\n",
    "class MyBmmRelu:\n",
    "  @T.prim_func\n",
    "  def bmm_relu(\n",
    "    A: T.Buffer[(16,128,128), \"float32\"],\n",
    "    B: T.Buffer[(16,128,128), \"float32\"],\n",
    "    C: T.Buffer[(16,128,128), \"float32\"]\n",
    "  ):\n",
    "    T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
    "    # TODO\n",
    "    Y = T.alloc_buffer((16, 128, 128), dtype=\"float32\")\n",
    "    for n, i, j, k in T.grid(16,128,128,128):\n",
    "        with T.block(\"Y\"):\n",
    "            vn = T.axis.spatial(16, n)\n",
    "            vi = T.axis.spatial(128, i)\n",
    "            vj = T.axis.spatial(128, j)\n",
    "            vk = T.axis.reduce(128, k)\n",
    "            with T.init():\n",
    "                Y[vn, vi, vj] = T.float32(0)\n",
    "            Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
    "    for n, i, j in T.grid(16,128,128):\n",
    "        with T.block(\"C\"):\n",
    "            vn = T.axis.spatial(16, n)\n",
    "            vi = T.axis.spatial(128, i)\n",
    "            vj = T.axis.spatial(128, j)\n",
    "            C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\n",
    "\n",
    "sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "print(sch.mod.script())\n",
    "# Also please validate your result\n",
    "a_tvm = tvm.nd.array(a_np)\n",
    "b_tvm = tvm.nd.array(b_np)\n",
    "c_tvm = tvm.nd.array(np.empty((16, 128, 128), dtype=np.float32))\n",
    "rt_lib = tvm.build(MyBmmRelu, target=\"llvm\")\n",
    "rt_lib[\"bmm_relu\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)\n",
    "\n",
    "# sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "# IPython.display.Code(sch.mod.script(), language=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换目标输出\n",
    "@tvm.script.ir_module\n",
    "class TargetModule:\n",
    "    @T.prim_func\n",
    "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
    "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
    "        for i0 in T.parallel(16):\n",
    "            for i1, i2_0 in T.grid(128, 16):\n",
    "                for ax0_init in T.vectorized(8):\n",
    "                    with T.block(\"Y_init\"):\n",
    "                        n, i = T.axis.remap(\"SS\", [i0, i1])\n",
    "                        j = T.axis.spatial(128, i2_0 * 8 + ax0_init)\n",
    "                        Y[n, i, j] = T.float32(0)\n",
    "                for ax1_0 in T.serial(32):\n",
    "                    for ax1_1 in T.unroll(4):\n",
    "                        for ax0 in T.serial(8):\n",
    "                            with T.block(\"Y_update\"):\n",
    "                                n, i = T.axis.remap(\"SS\", [i0, i1])\n",
    "                                j = T.axis.spatial(128, i2_0 * 8 + ax0)\n",
    "                                k = T.axis.reduce(128, ax1_0 * 4 + ax1_1)\n",
    "                                Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
    "                for i2_1 in T.vectorized(8):\n",
    "                    with T.block(\"C\"):\n",
    "                        n, i = T.axis.remap(\"SS\", [i0, i1])\n",
    "                        j = T.axis.spatial(128, i2_0 * 8 + i2_1)\n",
    "                        C[n, i, j] = T.max(Y[n, i, j], T.float32(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n in T.parallel(16):\n",
      "            for i, j, k in T.grid(128, 128, 128):\n",
      "                with T.block(\"Y\"):\n",
      "                    vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
      "                    T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                    T.writes(Y[vn, vi, vj])\n",
      "                    with T.init():\n",
      "                        Y[vn, vi, vj] = T.float32(0)\n",
      "                    Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "        for n, i, j in T.grid(16, 128, 128):\n",
      "            with T.block(\"C\"):\n",
      "                vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                T.reads(Y[vn, vi, vj])\n",
      "                T.writes(C[vn, vi, vj])\n",
      "                C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n in T.parallel(16):\n",
      "            for i, j_0, k, j_1 in T.grid(128, 16, 128, 8):\n",
      "                with T.block(\"Y\"):\n",
      "                    vn, vi = T.axis.remap(\"SS\", [n, i])\n",
      "                    vj = T.axis.spatial(128, j_0 * 8 + j_1)\n",
      "                    vk = T.axis.reduce(128, k)\n",
      "                    T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                    T.writes(Y[vn, vi, vj])\n",
      "                    with T.init():\n",
      "                        Y[vn, vi, vj] = T.float32(0)\n",
      "                    Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "        for n, i, j in T.grid(16, 128, 128):\n",
      "            with T.block(\"C\"):\n",
      "                vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
      "                T.reads(Y[vn, vi, vj])\n",
      "                T.writes(C[vn, vi, vj])\n",
      "                C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n in T.parallel(16):\n",
      "            for i, j_0 in T.grid(128, 16):\n",
      "                for k, j_1 in T.grid(128, 8):\n",
      "                    with T.block(\"Y\"):\n",
      "                        vn, vi = T.axis.remap(\"SS\", [n, i])\n",
      "                        vj = T.axis.spatial(128, j_0 * 8 + j_1)\n",
      "                        vk = T.axis.reduce(128, k)\n",
      "                        T.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                        T.writes(Y[vn, vi, vj])\n",
      "                        with T.init():\n",
      "                            Y[vn, vi, vj] = T.float32(0)\n",
      "                        Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "                for ax0 in T.serial(8):\n",
      "                    with T.block(\"C\"):\n",
      "                        vn, vi = T.axis.remap(\"SS\", [n, i])\n",
      "                        vj = T.axis.spatial(128, j_0 * 8 + ax0)\n",
      "                        T.reads(Y[vn, vi, vj])\n",
      "                        T.writes(C[vn, vi, vj])\n",
      "                        C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n",
      "# from tvm.script import tir as T\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @T.prim_func\n",
      "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with T.block(\"root\")\n",
      "        Y = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n in T.parallel(16):\n",
      "            for i, j_0 in T.grid(128, 16):\n",
      "                for j_1_init in T.vectorized(8):\n",
      "                    with T.block(\"Y_init\"):\n",
      "                        vn, vi = T.axis.remap(\"SS\", [n, i])\n",
      "                        vj = T.axis.spatial(128, j_0 * 8 + j_1_init)\n",
      "                        T.reads()\n",
      "                        T.writes(Y[vn, vi, vj])\n",
      "                        Y[vn, vi, vj] = T.float32(0)\n",
      "                for k_0 in T.serial(32):\n",
      "                    for k_1 in T.unroll(4):\n",
      "                        for j_1 in T.serial(8):\n",
      "                            with T.block(\"Y_update\"):\n",
      "                                vn, vi = T.axis.remap(\"SS\", [n, i])\n",
      "                                vj = T.axis.spatial(128, j_0 * 8 + j_1)\n",
      "                                vk = T.axis.reduce(128, k_0 * 4 + k_1)\n",
      "                                T.reads(Y[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])\n",
      "                                T.writes(Y[vn, vi, vj])\n",
      "                                Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "                for ax0 in T.vectorized(8):\n",
      "                    with T.block(\"C\"):\n",
      "                        vn, vi = T.axis.remap(\"SS\", [n, i])\n",
      "                        vj = T.axis.spatial(128, j_0 * 8 + ax0)\n",
      "                        T.reads(Y[vn, vi, vj])\n",
      "                        T.writes(C[vn, vi, vj])\n",
      "                        C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\n",
      "    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sch = tvm.tir.Schedule(MyBmmRelu)\n",
    "# TODO: transformations\n",
    "# Hints: you can use\n",
    "# `IPython.display.Code(sch.mod.script(), language=\"python\")`\n",
    "# or `print(sch.mod.script())`\n",
    "# to show the current program at any time during the transformation.\n",
    "\n",
    "# Step 1. Get blocks\n",
    "Y = sch.get_block(\"Y\", func_name=\"bmm_relu\")\n",
    "...\n",
    "\n",
    "# Step 2. Get loops\n",
    "b, i, j, k = sch.get_loops(Y)\n",
    "sch.parallel(b)\n",
    "print(sch.mod.script())\n",
    "\n",
    "# Step 3. Organize the loops\n",
    "j0, j1 = sch.split(j, factors=[None, 8])\n",
    "sch.reorder(j0,k, j1)\n",
    "print(sch.mod.script())\n",
    "block_C = sch.get_block(\"C\", func_name=\"bmm_relu\")\n",
    "sch.reverse_compute_at(block_C, j0)\n",
    "print(sch.mod.script())\n",
    "\n",
    "# Step 4. decompose reduction\n",
    "Y_init = sch.decompose_reduction(Y, k)\n",
    "n, i, j_0, j_1_init = sch.get_loops(Y_init)\n",
    "_, _, _, ax0 = sch.get_loops(block_C)\n",
    "Y_update_block = sch.get_block(\"Y_update\", func_name=\"bmm_relu\")\n",
    "_, _, _, k, j_1 = sch.get_loops(Y_update_block)\n",
    "k0, k1 = sch.split(k, factors=[32, 4])\n",
    "\n",
    "\n",
    "# # Step 5. vectorize / parallel / unroll\n",
    "sch.vectorize(j_1_init)\n",
    "sch.vectorize(ax0)\n",
    "sch.unroll(k1)\n",
    "\n",
    "print(sch.mod.script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "# 验证输出相同\n",
    "tvm.ir.assert_structural_equal(sch.mod, TargetModule)\n",
    "print(\"Pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transformation:\n",
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "  47.4496      47.4496      47.4496      47.4496       0.0000   \n",
      "               \n",
      "After transformation:\n",
      "Execution time summary:\n",
      " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
      "   3.9868       3.9868       3.9868       3.9868       0.0000   \n",
      "               \n"
     ]
    }
   ],
   "source": [
    "before_rt_lib = tvm.build(MyBmmRelu, target=\"llvm\")\n",
    "after_rt_lib = tvm.build(sch.mod, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(np.random.rand(16, 128, 128).astype(\"float32\"))\n",
    "b_tvm = tvm.nd.array(np.random.rand(16, 128, 128).astype(\"float32\"))\n",
    "c_tvm = tvm.nd.array(np.random.rand(16, 128, 128).astype(\"float32\"))\n",
    "after_rt_lib[\"bmm_relu\"](a_tvm, b_tvm, c_tvm)\n",
    "before_timer = before_rt_lib.time_evaluator(\"bmm_relu\", tvm.cpu())\n",
    "print(\"Before transformation:\")\n",
    "print(before_timer(a_tvm, b_tvm, c_tvm))\n",
    "\n",
    "f_timer = after_rt_lib.time_evaluator(\"bmm_relu\", tvm.cpu())\n",
    "print(\"After transformation:\")\n",
    "print(f_timer(a_tvm, b_tvm, c_tvm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
